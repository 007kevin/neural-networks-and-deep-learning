#+title: Exercises

* Exercise - [[http://neuralnetworksanddeeplearning.com/chap1.html#exercises_191892][link]]

** Sigmoid neurons simulating perceptrons, part I
Question:
Suppose we take all the weights and biases in a network of perceptrons, and
multiply them by a positive constant, $c>0$. Show that the behaviour of the
network doesn't change.

Solution:
The definition of a perceptron with bias is

\begin{eqnarray*}
  \mbox{output} = \left\{
    \begin{array}{ll}
      0 & \mbox{if } w\cdot x + b \leq 0 \\
      1 & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right.
\tag{2}\end{eqnarray*}


multiplied with positive constant c gives us

\begin{eqnarray*}
  \mbox{output} = \left\{
    \begin{array}{ll}
      0 & \mbox{if } cw\cdot x + cb \leq 0 \\
      1 & \mbox{if } cw\cdot x + cb > 0
    \end{array}
  \right.
\tag{2}\end{eqnarray*}

\begin{eqnarray*}
  \mbox{output} = \left\{
    \begin{array}{ll}
      0 & \mbox{if } c(w\cdot x + b) \leq 0 \\
      1 & \mbox{if } c(w\cdot x + b) > 0
    \end{array}
  \right.
\tag{2}\end{eqnarray*}

\begin{eqnarray*}
  \mbox{output} = \left\{
    \begin{array}{ll}
      0 & \mbox{if } w\cdot x + b \leq 0 \\
      1 & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right
\tag{2}\end{eqnarray*}

which does not change the behaviour of the network.

** Sigmoid neurons simulating perceptrons, part II
Question:
Suppose we have the same setup as the last problem - a network of perceptrons.
Suppose also that the overall input to the network of perceptrons has been
chosen. We won't need the actual input value, we just need the input to have
been fixed. Suppose the weights and biases are such that w⋅x+b≠0 for the input x
to any particular perceptron in the network. Now replace all the perceptrons in
the network by sigmoid neurons, and multiply the weights and biases by a
positive constant c>0. Show that in the limit as c→∞ the behaviour of this
network of sigmoid neurons is exactly the same as the network of perceptrons.
How can this fail when w⋅x+b=0 for one of the perceptrons?

Solution:
A sigmoid is defined by

\begin{eqnarray*}
  \sigma(z) \equiv \frac{1}{1+e^{-z}}
\end{eqnarray*}

so a sigmoid neuron becomes

\begin{eqnarray*}
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}
\end{eqnarray*}

Supposing the weights and biases are such that w⋅x+b≠0, multiplying them by a
positive constant c gives us

\begin{eqnarray*}
  \frac{1}{1+\exp(-\sum_j cw_j x_j-cb)}
\end{eqnarray*}

in the limit c→∞

\begin{align*}
  \lim_{c \to \infty} \frac{1}{1+\exp(-\sum_j cw_j x_j-cb)}
&= \lim_{c \to \infty} \frac{1}{1+\exp(-\sum_j c(w_j x_j-b)} \\
&= \lim_{c \to \infty} \frac{1}{1+\exp(-\sum_j c(w_j x_j-b)} \\
&= \left\{
    \begin{array}{ll}
      \lim_{c \to \infty} \frac{1}{1+\exp(\infty)} & \mbox{if } w\cdot x + b \leq 0 \\
      \lim_{c \to \infty} \frac{1}{1+\exp(-\infty)} & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right \\
&= \left\{
    \begin{array}{ll}
      \frac{1}{1+\infty} & \mbox{if } w\cdot x + b \leq 0 \\
      \frac{1}{1+0} & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right \\
&= \left\{
    \begin{array}{ll}
      0 & \mbox{if } w\cdot x + b \leq 0 \\
      1 & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right \\
\end{align*}

this can fail when w⋅x+b=0 because

\begin{align*}
\frac{1}{1+\exp(0)} = \frac{1}{1+1} = \frac{1}{2}
\end{align*}

* Exercise - [[http://neuralnetworksanddeeplearning.com/chap1.html#exercise_513527][link]]
** Determining bitwise representation of a digit

third layer have activation at least 0.99. incorrect has 0.01

| decimal | binary |
|---------+--------|
|       0 |   0000 |
|       1 |   0001 |
|       2 |   0010 |
|       3 |   0011 |
|       4 |   0100 |
|       5 |   0101 |
|       6 |   0110 |
|       7 |   0111 |
|       8 |   1000 |
|       9 |   1001 |

sigmoid definition

\begin{eqnarray*}
  \sigma(\sum_j w_j x_j+b)
\end{eqnarray*}

which is the sum of all weights plus the bias

#+begin_src python
import math

def sigm(x):
    return 1 / (1 + math.exp(-x))

def f(actvns, weights, bias):
    sum=0;
    for i, a in enumerate(actvns):
        sum+=a*weights[i]
    return 0 if sigm(sum + bias) < 0.5 else 1

def calculate(num):
    actvns=[0.99 if x == num else 0.01 for x in range(10)]

    o3 = f(actvns, [-1, 1,-1, 1,-1, 1,-1, 1,-1, 1], 0);
    o2 = f(actvns, [-1,-1, 1, 1,-1,-1, 1, 1,-1,-1], 0);
    o1 = f(actvns, [-1,-1,-1,-1, 1, 1, 1, 1,-1,-1], 0);
    o0 = f(actvns, [-1,-1,-1,-1,-1,-1,-1,-1, 1, 1], 0);
    return str(o0) + str(o1) + str(o2) + str(o3)

output = ""
for i in range(10):
    output += str(i) + ": " + calculate(i) + "\n";

return output
#+end_src

#+RESULTS:
#+begin_example
0: 0000
1: 0001
2: 0010
3: 0011
4: 0100
5: 0101
6: 0110
7: 0111
8: 1000
9: 1001
#+end_example

* Exercise - [[http://neuralnetworksanddeeplearning.com/chap1.html#exercises_647181][link]]
** Part I - Prove the assertion
Lets suppose that were trying to make a move $\Delta v$ in position so as to
decrease $C$ as much as possible. This is the same as minimizing

\begin{eqnarray*}
  \Delta C \approx \nabla C\cdot \Delta v
\end{eqnarray*}

because we want $\Delta C \leq 0$ as minimal as possible to reduce the cost function.

Prove that when we constrain the size of the move $\|\Delta v \| = \epsilon$, for some fixed
$\epsilon > 0$ that the choice of $\Delta v$ that minimizes $\nabla C\cdot \Delta v$ is

\begin{eqnarray*}
  \Delta v = - \eta \nabla C
\end{eqnarray*}

where

\begin{eqnarray*}
  \eta = \epsilon / \|\nabla C\|
\end{eqnarray*}

*** Proof
... solved on paper.


** Part II
What happens when C is a function of just one variable? Can you provide a geometric
interpretation of what gradient descent is doing in the one-dimensional case?

In the one dimensional case, the gradient descent becomes the derivative of the cost function:

\begin{eqnarray*}
  \nabla C(x) = C'(x)
\end{eqnarray*}

and

\begin{eqnarray*}
  \Delta v(x) &=-\eta\nabla C \\
              &=-\frac{\epsilon}{\lVert \nabla C \rVert} \nabla C \\
              &=-\frac{\epsilon}{C'(x)} C'(x) \\
              &=-\epsilon
\end{eqnarray*}

note the magnitude of a 1D vector is itself.

so at every step we're moving

* Exercise - [[http://neuralnetworksanddeeplearning.com/chap1.html#exercise_263792][link]]

estimating the gradient vector using mini-batch of size $m$

\begin{eqnarray*}
  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C
\end{eqnarray*}


|                     | advantage                                | disadvantage                                  |
|---------------------+------------------------------------------+-----------------------------------------------|
| online learning     | can learn from single data point         | takes more epochs to find local minimum       |
| stochastic gradient | better at estimating the gradient vector | must wait until m data points before learning |
|                     |                                          |                                               |
